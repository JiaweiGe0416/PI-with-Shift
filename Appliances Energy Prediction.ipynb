{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f500a949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F # All functions that don't have any parameters\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "from scipy.stats import iqr\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "\n",
    "\n",
    "\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as lines\n",
    "\n",
    "import ot\n",
    "\n",
    "import warnings\n",
    "random_seed = 42\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"GPU is not available.\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c205a1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 374, 'name': 'Appliances Energy Prediction', 'repository_url': 'https://archive.ics.uci.edu/dataset/374/appliances+energy+prediction', 'data_url': 'https://archive.ics.uci.edu/static/public/374/data.csv', 'abstract': 'Experimental data used to create regression models of appliances energy use in a low energy building.', 'area': 'Computer Science', 'tasks': ['Regression'], 'characteristics': ['Multivariate', 'Time-Series'], 'num_instances': 19735, 'num_features': 28, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['Appliances'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2017, 'last_updated': 'Fri Mar 29 2024', 'dataset_doi': '10.24432/C5VC8G', 'creators': ['Luis Candanedo'], 'intro_paper': {'title': 'Data driven prediction models of energy use of appliances in a low-energy house', 'authors': 'L. Candanedo, V. Feldheim, Dominique Deramaix', 'published_in': 'Energy and Buildings, Volume 140', 'year': 2017, 'url': 'https://www.semanticscholar.org/paper/28025ec6f4d5ab121bd91b36c754c0f0831d2433', 'doi': '10.1016/j.enbuild.2017.01.083'}, 'additional_info': {'summary': 'The data set is at 10 min for about 4.5 months. The house temperature and humidity conditions were monitored with a ZigBee wireless sensor network. Each wireless node transmitted the temperature and humidity conditions around 3.3 min. Then, the wireless data was averaged for 10 minutes periods. The energy data was logged every 10 minutes with m-bus energy meters. Weather from the nearest airport weather station (Chievres Airport, Belgium) was downloaded from a public data set from Reliable Prognosis (rp5.ru), and merged together with the experimental data sets using the date and time column. Two random variables have been included in the data set for testing the regression models and to filter out non predictive attributes (parameters).\\r\\n\\r\\nFor more information about the house, data collection, R scripts and figures, please refer to the paper and to the following github repository:\\r\\n\\r\\nhttps://github.com/LuisM78/Appliances-energy-prediction-data\\r\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'date time year-month-day hour:minute:second \\r\\nAppliances, energy use in Wh\\r\\nlights, energy use of light fixtures in the house in Wh\\r\\nT1, Temperature in kitchen area, in Celsius\\r\\nRH_1, Humidity in kitchen area, in %\\r\\nT2, Temperature in living room area, in Celsius\\r\\nRH_2, Humidity in living room area, in %\\r\\nT3, Temperature in laundry room area\\r\\nRH_3, Humidity in laundry room area, in %\\r\\nT4, Temperature in office room, in Celsius\\r\\nRH_4, Humidity in office room, in %\\r\\nT5, Temperature in bathroom, in Celsius\\r\\nRH_5, Humidity in bathroom, in %\\r\\nT6, Temperature outside the building (north side), in Celsius\\r\\nRH_6, Humidity outside the building (north side), in %\\r\\nT7, Temperature in ironing room , in Celsius\\r\\nRH_7, Humidity in ironing room, in %\\r\\nT8, Temperature in teenager room 2, in Celsius\\r\\nRH_8, Humidity in teenager room 2, in %\\r\\nT9, Temperature in parents room, in Celsius\\r\\nRH_9, Humidity in parents room, in %\\r\\nTo, Temperature outside (from Chievres weather station), in Celsius\\r\\nPressure (from Chievres weather station), in mm Hg\\r\\nRH_out, Humidity outside (from Chievres weather station), in %\\r\\nWind speed (from Chievres weather station), in m/s\\r\\nVisibility (from Chievres weather station), in km\\r\\nTdewpoint (from Chievres weather station), Â°C\\r\\nrv1, Random variable 1, nondimensional\\r\\nrv2, Random variable 2, nondimensional\\r\\n\\r\\nWhere indicated, hourly data (then interpolated) from the nearest airport weather station (Chievres Airport, Belgium) was downloaded from a public data set from Reliable Prognosis, rp5.ru. Permission was obtained from Reliable Prognosis for the distribution of the 4.5 months of weather data.\\r\\n', 'citation': None}}\n",
      "           name     role        type demographic description  units  \\\n",
      "0          date  Feature        Date        None        None   None   \n",
      "1    Appliances   Target     Integer        None        None     Wh   \n",
      "2        lights  Feature     Integer        None        None     Wh   \n",
      "3            T1  Feature  Continuous        None        None      C   \n",
      "4          RH_1  Feature  Continuous        None        None      %   \n",
      "5            T2  Feature  Continuous        None        None      C   \n",
      "6          RH_2  Feature  Continuous        None        None      %   \n",
      "7            T3  Feature  Continuous        None        None      C   \n",
      "8          RH_3  Feature  Continuous        None        None      %   \n",
      "9            T4  Feature  Continuous        None        None      C   \n",
      "10         RH_4  Feature  Continuous        None        None      %   \n",
      "11           T5  Feature  Continuous        None        None      C   \n",
      "12         RH_5  Feature  Continuous        None        None      %   \n",
      "13           T6  Feature  Continuous        None        None      C   \n",
      "14         RH_6  Feature  Continuous        None        None      %   \n",
      "15           T7  Feature  Continuous        None        None      C   \n",
      "16         RH_7  Feature  Continuous        None        None      %   \n",
      "17           T8  Feature  Continuous        None        None      C   \n",
      "18         RH_8  Feature  Continuous        None        None      %   \n",
      "19           T9  Feature  Continuous        None        None      C   \n",
      "20         RH_9  Feature  Continuous        None        None      %   \n",
      "21        T_out  Feature  Continuous        None        None      C   \n",
      "22  Press_mm_hg  Feature  Continuous        None        None  mm Hg   \n",
      "23       RH_out  Feature  Continuous        None        None      %   \n",
      "24    Windspeed  Feature  Continuous        None        None    m/s   \n",
      "25   Visibility  Feature  Continuous        None        None     km   \n",
      "26    Tdewpoint  Feature  Continuous        None        None      C   \n",
      "27          rv1  Feature  Continuous        None        None   None   \n",
      "28          rv2  Feature  Continuous        None        None   None   \n",
      "\n",
      "   missing_values  \n",
      "0              no  \n",
      "1              no  \n",
      "2              no  \n",
      "3              no  \n",
      "4              no  \n",
      "5              no  \n",
      "6              no  \n",
      "7              no  \n",
      "8              no  \n",
      "9              no  \n",
      "10             no  \n",
      "11             no  \n",
      "12             no  \n",
      "13             no  \n",
      "14             no  \n",
      "15             no  \n",
      "16             no  \n",
      "17             no  \n",
      "18             no  \n",
      "19             no  \n",
      "20             no  \n",
      "21             no  \n",
      "22             no  \n",
      "23             no  \n",
      "24             no  \n",
      "25             no  \n",
      "26             no  \n",
      "27             no  \n",
      "28             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "appliances_energy_prediction = fetch_ucirepo(id=374) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = appliances_energy_prediction.data.features \n",
    "y = appliances_energy_prediction.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(appliances_energy_prediction.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(appliances_energy_prediction.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e0219488",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = X.to_numpy()\n",
    "y_np = y.to_numpy().astype('float64')\n",
    "X_np = X_np[:,1:]\n",
    "date = X.to_numpy()[:,0]\n",
    "\n",
    "X_np = X_np.astype('float64')\n",
    "#y_np /= np.max(y_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2f7acb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_np[:5000,:]\n",
    "X_test = X_np[-2000:,:]\n",
    "\n",
    "Y_train = y_np[:5000,:]\n",
    "Y_test = y_np[-2000:,:]\n",
    "\n",
    "Dtrain = np.hstack((X_train, Y_train))\n",
    "Dshift = np.hstack((X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4324172f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2016-05-1320:50:00', '2016-05-1321:00:00', '2016-05-1321:10:00',\n",
       "       ..., '2016-05-2717:40:00', '2016-05-2717:50:00',\n",
       "       '2016-05-2718:00:00'], dtype=object)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date[-2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3404b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN1(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NN1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 10)\n",
    "        self.fc2 = nn.Linear(10, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# 2-layer NN\n",
    "class NN2(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NN2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x \n",
    "    \n",
    "    \n",
    "    \n",
    "class QuantileLoss(nn.Module):\n",
    "    def __init__(self, quantile):\n",
    "        super().__init__()\n",
    "        self.quantile = quantile\n",
    "\n",
    "    def forward(self, preds, target):\n",
    "        assert not target.requires_grad\n",
    "        assert preds.size(0) == target.size(0)\n",
    "        residuals = target - preds\n",
    "        return torch.max((self.quantile - 1) * residuals, self.quantile * residuals).mean()\n",
    "        \n",
    "        \n",
    "def est_quantile(est_type,quantile,X_pre,Y_pre,X_opt,X_adj,X_t):\n",
    "    \"\"\"\n",
    "        est_type: \n",
    "        \"NN1\": 1-layer NN;              \"NN2\": 2-layer NN; \n",
    "        \"qrf\": quantile regression forest;    \"gb\": gradient boosting\n",
    "        \n",
    "        quantile: the quantile we are estimating\n",
    "        (X_pre,Y_pre): training data\n",
    "        X_opt,X_adj,X_t: data used to predict\n",
    "        output: quantile estimator Q and the prediction Q(X) \n",
    "    \"\"\"\n",
    "    \n",
    "    num_var = X_pre.shape[1]\n",
    "    \n",
    "    if est_type == \"NN1\":\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "        \n",
    "        # Convert numpy arrays to torch tensors\n",
    "        X = torch.from_numpy(X_pre).float().to(device)\n",
    "        Y = torch.from_numpy(Y_pre).float().to(device)\n",
    "        \n",
    "        model = NN1(input_size=num_var, output_size=1).to(device)\n",
    "        learning_rate = 0.001\n",
    "        \n",
    "        # Set loss function and optimizer\n",
    "        criterion = QuantileLoss(quantile).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(1000):\n",
    "            optimizer.zero_grad()   # zero the gradient buffers\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()    # Does the update\n",
    "            \n",
    "        # Predict\n",
    "        X_opt = torch.from_numpy(X_opt).float().to(device)\n",
    "        X_adj = torch.from_numpy(X_adj).float().to(device)\n",
    "        X_t = torch.from_numpy(X_t).float().to(device)\n",
    "\n",
    "        Q_opt = model(X_opt)\n",
    "        Q_opt = Q_opt.detach().cpu().numpy().reshape(-1,1)\n",
    "        Q_adj = model(X_adj)\n",
    "        Q_adj = Q_adj.detach().cpu().numpy().reshape(-1,1)\n",
    "        Q_t = model(X_t)\n",
    "        Q_t = Q_t.detach().cpu().numpy().reshape(-1,1)\n",
    "        return model, Q_opt, Q_adj, Q_t\n",
    "    \n",
    "    if est_type == \"NN2\":\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42) \n",
    "        \n",
    "        # Convert numpy arrays to torch tensors\n",
    "        X = torch.from_numpy(X_pre).float().to(device)\n",
    "        Y = torch.from_numpy(Y_pre).float().to(device)\n",
    "        \n",
    "        model = NN2(input_size=num_var, output_size=1).to(device)\n",
    "        learning_rate = 0.001\n",
    "        \n",
    "        # Set loss function and optimizer\n",
    "        criterion = QuantileLoss(quantile).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(1000):\n",
    "            optimizer.zero_grad()   # zero the gradient buffers\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()    # Does the update\n",
    "            \n",
    "        # Predict\n",
    "        X_opt = torch.from_numpy(X_opt).float().to(device)\n",
    "        X_adj = torch.from_numpy(X_adj).float().to(device)\n",
    "        X_t = torch.from_numpy(X_t).float().to(device)\n",
    "\n",
    "        Q_opt = model(X_opt)\n",
    "        Q_opt = Q_opt.detach().cpu().numpy().reshape(-1,1)\n",
    "        Q_adj = model(X_adj)\n",
    "        Q_adj = Q_adj.detach().cpu().numpy().reshape(-1,1)\n",
    "        Q_t = model(X_t)\n",
    "        Q_t = Q_t.detach().cpu().numpy().reshape(-1,1)\n",
    "        return model, Q_opt, Q_adj, Q_t\n",
    "    \n",
    "    if est_type == \"qrf\":\n",
    "        model = RandomForestQuantileRegressor(n_estimators = 500, random_state=random_seed)\n",
    "        model.fit(X_pre, Y_pre)\n",
    "        Q_opt = model.predict(X_opt,quantiles = [quantile]).reshape(-1,1)\n",
    "        Q_adj = model.predict(X_adj,quantiles = [quantile]).reshape(-1,1)\n",
    "        Q_t = model.predict(X_t,quantiles = [quantile]).reshape(-1,1)\n",
    "        return model, Q_opt, Q_adj, Q_t\n",
    "    \n",
    "    \n",
    "    if est_type == \"gb\":\n",
    "        model = GradientBoostingRegressor(n_estimators=300,random_state=random_seed,loss = \"quantile\", alpha = quantile)\n",
    "        model.fit(X_pre, Y_pre)\n",
    "        Q_opt = model.predict(X_opt).reshape(-1,1)\n",
    "        Q_adj = model.predict(X_adj).reshape(-1,1)\n",
    "        Q_t = model.predict(X_t).reshape(-1,1)\n",
    "        return model, Q_opt, Q_adj, Q_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ac810cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_utopia(mc_iter):\n",
    "    Y_train = Dtrain[:,-1].astype('float64')\n",
    "    X_train = Dtrain[:,:-1].astype('float64')\n",
    "\n",
    "    X_test = Dshift[:,:-1].astype('float64')\n",
    "    Y_test = Dshift[:,-1].astype('float64')\n",
    "    \n",
    "    pre_idx = int(Dtrain.shape[0] * 0.5)\n",
    "    Dtrain_pre, Dtrain_opt = np.split(Dtrain, [pre_idx])\n",
    "    adj_idx = int(Dtrain_opt.shape[0] * 0.5)\n",
    "    Dtrain_opt, Dtrain_adj = np.split(Dtrain_opt, [adj_idx])\n",
    "\n",
    "    X_pre = Dtrain_pre[:,:-1].astype('float64')\n",
    "    Y_pre = Dtrain_pre[:,-1].astype('float64')\n",
    "\n",
    "    X_opt = Dtrain_opt[:,:-1].astype('float64')\n",
    "    Y_opt = Dtrain_opt[:,-1].astype('float64')\n",
    "\n",
    "    X_adj = Dtrain_adj[:,:-1].astype('float64')\n",
    "    Y_adj = Dtrain_adj[:,-1].astype('float64')\n",
    "    \n",
    "    #mean_model = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "    mean_model = LinearRegression()\n",
    "    mean_model.fit(X_pre, Y_pre)\n",
    "\n",
    "    mean_est_pre = mean_model.predict(X_pre)\n",
    "    mean_est_opt = mean_model.predict(X_opt)\n",
    "    mean_est_adj = mean_model.predict(X_adj)\n",
    "\n",
    "    Y_centered_squared_pre = (Y_pre - mean_est_pre)**2 \n",
    "    Y_centered_squared_opt = (Y_opt - mean_est_opt)**2 \n",
    "    Y_centered_squared_adj = (Y_adj - mean_est_adj)**2 \n",
    "    \n",
    "    \n",
    "    ######## Estimation of the shift ###########\n",
    "#     ot_sinkhorn = ot.da.SinkhornTransport(reg_e=1e-1, max_iter=1000)\n",
    "#     ot_sinkhorn.fit(Xs=X_test, Xt=X_pre)\n",
    "#     X_test = ot_sinkhorn.transform(Xs=X_test)\n",
    "    \n",
    "    ot_emd = ot.da.EMDTransport()\n",
    "    ot_emd.fit(Xs=X_test, Xt=X_pre)\n",
    "    X_test = ot_emd.transform(Xs=X_test)\n",
    "    \n",
    "#     ot_emd = ot.da.LinearTransport()\n",
    "#     ot_emd.fit(Xs=X_test, Xt=X_pre)\n",
    "#     X_test = ot_emd.transform(Xs=X_test)\n",
    "    \n",
    "    \n",
    "    quantile = [0.85,0.95,0.9,0.9]\n",
    "\n",
    "\n",
    "    m1,Q1_opt,Q1_adj,Q1_t = est_quantile(\"NN1\",quantile[0],X_pre,Y_centered_squared_pre,X_opt,X_adj,X_test) # quantile def\n",
    "    m2,Q2_opt,Q2_adj,Q2_t = est_quantile(\"NN2\",quantile[1],X_pre,Y_centered_squared_pre,X_opt,X_adj,X_test)\n",
    "    m3,Q3_opt,Q3_adj,Q3_t = est_quantile(\"qrf\",quantile[2],X_pre,Y_centered_squared_pre,X_opt,X_adj,X_test)\n",
    "    m4,Q4_opt,Q4_adj,Q4_t = est_quantile(\"gb\",quantile[3],X_pre,Y_centered_squared_pre,X_opt,X_adj,X_test)\n",
    "    \n",
    "\n",
    "    ######## Variance estimator ###########\n",
    "    \n",
    "    rf_var_model = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "    rf_var_model.fit(X_pre, Y_centered_squared_pre)\n",
    "    var_hat_pre = rf_var_model.predict(X_pre)\n",
    "    var_hat_adj = rf_var_model.predict(X_adj)\n",
    "    var_hat_opt = rf_var_model.predict(X_opt)\n",
    "    var_hat_test = rf_var_model.predict(X_test)\n",
    "    \n",
    "    E_opt = np.vstack((np.matrix.flatten(Q1_opt), np.matrix.flatten(Q2_opt), np.matrix.flatten(Q3_opt), np.matrix.flatten(Q4_opt), var_hat_opt))\n",
    "    E_adj = np.vstack((np.matrix.flatten(Q1_adj), np.matrix.flatten(Q2_adj), np.matrix.flatten(Q3_adj), np.matrix.flatten(Q4_adj), var_hat_adj))\n",
    "    E_test = np.vstack((np.matrix.flatten(Q1_t), np.matrix.flatten(Q2_t), np.matrix.flatten(Q3_t), np.matrix.flatten(Q4_t), var_hat_test))\n",
    "    \n",
    "    n_opt = X_opt.shape[0]\n",
    "    n_adj = X_adj.shape[0]\n",
    "    n_test = X_test.shape[0]\n",
    "\n",
    "    cons_opt = np.ones(n_opt).reshape(1,-1)\n",
    "    cons_adj = np.ones(n_adj).reshape(1,-1)\n",
    "    cons_test = np.ones(n_test).reshape(1,-1)\n",
    "\n",
    "    A_opt = np.vstack((E_opt,cons_opt))\n",
    "    A_adj = np.vstack((E_adj,cons_adj))\n",
    "    A_test = np.vstack((E_test,cons_test))\n",
    "\n",
    "    weight = cp.Variable(A_opt.shape[0])\n",
    "\n",
    "\n",
    "    constraints = [weight>=0]+[weight @ A_opt >= Y_centered_squared_opt]  ### Change the objective \n",
    "    prob = cp.Problem(cp.Minimize(cp.sum(weight @ A_opt)), constraints)\n",
    "    prob.solve()\n",
    "    optimal_weight = weight.value\n",
    "\n",
    "    f_hat_init_opt = optimal_weight @  A_opt\n",
    "    f_hat_init_adj = optimal_weight @  A_adj\n",
    "    f_hat_init_test = optimal_weight @ A_test\n",
    "\n",
    "        \n",
    "    \n",
    "    alpha = 0.05\n",
    "    scores_temp = Y_centered_squared_adj/f_hat_init_adj\n",
    "    level = (1 - alpha) * 100\n",
    "    level = int(level)\n",
    "    delta = np.percentile(scores_temp, level)\n",
    "    #delta = np.max(scores_temp)\n",
    "    \n",
    "    \n",
    "    mean_est_test = mean_model.predict(X_test)\n",
    "    Y_centered_squared_test = (Y_test - mean_est_test)**2 \n",
    "    cov = np.mean(Y_centered_squared_test < (delta * f_hat_init_test))\n",
    "    bw = 2 * np.mean((delta * f_hat_init_test) ** 0.5)\n",
    "    return cov, bw, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "087ea5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdeb/anaconda3/lib/python3.10/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9475, 461.6885627939844, 0.12669926011439528)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_utopia(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ed2ead80",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Weighted variance adjusted split conformal #######################################\n",
    "\n",
    "def prob_est(Dtrain, Dshift, est_type = 'lr'):\n",
    "    X_S = Dtrain[:,:-1]\n",
    "    X_T = Dshift[:,:-1]\n",
    "    n_S = X_S.shape[0]\n",
    "    n_T = X_T.shape[0]\n",
    "    Y_S = np.zeros(n_S).reshape(-1,1)\n",
    "    Y_T = np.ones(n_T).reshape(-1,1)\n",
    "    X_train = np.vstack((X_S, X_T))\n",
    "    Y_train = np.vstack((Y_S, Y_T))\n",
    "  \n",
    "    if est_type == 'lr':\n",
    "        # fit logistic regression\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            logistic_model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000,random_state=0) \n",
    "            logistic_model.fit(X_train, Y_train[:,0])\n",
    "\n",
    "        return logistic_model\n",
    "    \n",
    "    if est_type == 'rf':\n",
    "        # fit random forest\n",
    "        rf_model = RandomForestClassifier(n_estimators = 100, random_state=random_seed)\n",
    "        rf_model.fit(X_train, Y_train[:,0])\n",
    "\n",
    "        return rf_model\n",
    "\n",
    "    \n",
    "def ratio_est(model, X):\n",
    "    \n",
    "        prob_1 = model.predict_proba(X)[:,1]\n",
    "        prob_1 = np.clip(prob_1, 0.01, 0.99)\n",
    "        est_ratio = prob_1/(1-prob_1)\n",
    "        \n",
    "        return est_ratio\n",
    "    \n",
    "def weighting_function(model, X_calib, x_test, x):\n",
    "    # Define weighting function here\n",
    "    # all the data should be array\n",
    "    ratio_calib = ratio_est(model, X_calib).sum()\n",
    "    ratio_test = ratio_est(model, x_test)\n",
    "    ratio_sum = ratio_calib + ratio_test\n",
    "    ratio_x = ratio_est(model, x)\n",
    "    return ratio_x/ratio_sum\n",
    "\n",
    "\n",
    "def weighted_quantile(values, weights, quantile):\n",
    "    \n",
    "    \"\"\" Compute the weighted quantile of a 1D numpy array.\n",
    "    \"\"\"\n",
    "    values = np.array(values)\n",
    "    weights = np.array(weights)\n",
    "    \n",
    "    sorter = np.argsort(values)\n",
    "    values = values[sorter]\n",
    "    weights = weights[sorter]\n",
    "    cumulative_weights = np.cumsum(weights)\n",
    "    if cumulative_weights[-1]>= quantile:\n",
    "        idx = np.argmax(cumulative_weights >= quantile)\n",
    "        return values[idx]\n",
    "    else:\n",
    "        return np.max(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ce41fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_conformal_cov_width(Dtrain, Dshift, alpha=0.05):\n",
    "    \n",
    "    n = Dshift.shape[0]\n",
    "    cov = []\n",
    "    width = []\n",
    "    \n",
    "    \n",
    "    prob_est_model = prob_est(Dtrain, Dshift, est_type = 'lr')\n",
    "    \n",
    "    # split weighted conformal\n",
    "    D = Dtrain\n",
    "    np.random.shuffle(D)\n",
    "    split_idx = int(D.shape[0] * 0.5)\n",
    "    D1, D2 = np.split(D, [split_idx])\n",
    "    X1 = D1[:,:-1]\n",
    "    y1 = D1[:,-1]\n",
    "    X2 = D2[:,:-1]\n",
    "    y2 = D2[:,-1]\n",
    "    \n",
    "    # Train the model on the first part of the data\n",
    "    model = LinearRegression()\n",
    "    model.fit(X1, y1)\n",
    "    \n",
    "    # Compute the model's predictions for the calibration set\n",
    "    y1_hat = model.predict(X1)\n",
    "    y_calib_pred = model.predict(X2)\n",
    "            \n",
    "    # Calculate the absolute errors on the calibration set\n",
    "    errors_calib = np.abs(y_calib_pred - y2)\n",
    "    \n",
    "    est_error_1 = (y1_hat - y1) ** 2\n",
    "    var_model = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "    var_model.fit(X1, est_error_1)\n",
    "    var_hat = var_model.predict(X2)\n",
    "    sd_calib_pred = var_hat ** 0.5\n",
    "    errors_calib = np.abs(y_calib_pred - y2)/np.clip(sd_calib_pred, 0.001, 1e10)\n",
    "    \n",
    "    for i in range(n):\n",
    "        x_test = Dshift[i,:-1]\n",
    "        x_test = np.array(x_test).reshape(1,-1)\n",
    "        y_test = Dshift[i,-1]\n",
    "    \n",
    "    \n",
    "        # Calculate the weights for the calibration set\n",
    "        weights_calib = weighting_function(prob_est_model, X2, x_test, X2)\n",
    "\n",
    "\n",
    "        # Calculate the weighted quantile of the errors\n",
    "        quantile = weighted_quantile(errors_calib, weights_calib, 1 - alpha)\n",
    "\n",
    "        # Now for the test\n",
    "        y_test_pred = model.predict(x_test)\n",
    "        sd_test_pred = var_model.predict(x_test) ** 0.5\n",
    "        \n",
    "        width.append(2*quantile*sd_test_pred)\n",
    "        cov.append((np.abs(y_test - y_test_pred)/sd_test_pred) <= quantile)\n",
    "\n",
    "    \n",
    "    return np.array(cov).mean(), np.array(width).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cc9abc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "def run_weighted_conformal(n):\n",
    "    cov, bw = weighted_conformal_cov_width(Dtrain, Dshift, alpha)\n",
    "    return cov, bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "916d8fd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "6809.872889489878\n"
     ]
    }
   ],
   "source": [
    "cov, bw = run_weighted_conformal(1)\n",
    "print(cov)\n",
    "print(bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "acd1f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_conformal_quantile(Dtrain, Dshift, alpha=0.05):\n",
    "    \n",
    "    n = Dshift.shape[0]\n",
    "    cov = []\n",
    "    width = []\n",
    "    \n",
    "    \n",
    "    prob_est_model = prob_est(Dtrain, Dshift, est_type = 'lr')\n",
    "    \n",
    "    # split weighted conformal\n",
    "    D = Dtrain\n",
    "    np.random.shuffle(D)\n",
    "    split_idx = int(D.shape[0] * 0.5)\n",
    "    D1, D2 = np.split(D, [split_idx])\n",
    "    X1 = D1[:,:-1]\n",
    "    y1 = D1[:,-1]\n",
    "    X2 = D2[:,:-1]\n",
    "    y2 = D2[:,-1]\n",
    "    \n",
    "    # Train the model on the first part of the data\n",
    "#     model_quantile = RandomForestQuantileRegressor(n_estimators = 500, random_state=random_seed)\n",
    "#     model_quantile.fit(X1, y1)\n",
    "    \n",
    "    qr_lb = QuantileRegressor(quantile=alpha/2, solver = 'highs')\n",
    "    qr_ub = QuantileRegressor(quantile = (1-alpha)/2, solver = 'highs')\n",
    "    \n",
    "    qr_lb.fit(X1, y1)\n",
    "    qr_ub.fit(X1, y1)\n",
    "    \n",
    "#     y2_lb_hat = model_quantile.predict(X2, quantiles = alpha/2)\n",
    "#     y2_ub_hat = model_quantile.predict(X2, quantiles = (1-alpha)/2)\n",
    "    \n",
    "    y2_lb_hat = qr_lb.predict(X2)\n",
    "    y2_ub_hat = qr_ub.predict(X2)\n",
    "        \n",
    "#     model_ub = QuantileRegressor(quantile = (1-alpha)/2, alpha = 1, fit_intercept=True, solver='highs', solver_options=None)\n",
    "#     model_ub.fit(X1, y1)\n",
    "    \n",
    "#     model_lb = QuantileRegressor(quantile = alpha/2, alpha = 1, fit_intercept=True, solver='highs', solver_options=None)\n",
    "#     model_lb.fit(X1, y1)\n",
    "    \n",
    "#     y2_lb_hat = model_lb.predict(X2)\n",
    "#     y2_ub_hat = model_ub.predict(X2)\n",
    "    \n",
    "    errors_calib = np.maximum(y2_lb_hat - y2, y2 - y2_ub_hat)\n",
    "    \n",
    "    for i in range(n):\n",
    "        x_test = Dshift[i,:-1]\n",
    "        x_test = np.array(x_test).reshape(1,-1)\n",
    "        y_test = Dshift[i,-1]\n",
    "    \n",
    "    \n",
    "        # Calculate the weights for the calibration set\n",
    "        weights_calib = weighting_function(prob_est_model, X2, x_test, X2)\n",
    "\n",
    "\n",
    "        # Calculate the weighted quantile of the errors\n",
    "        quantile = weighted_quantile(errors_calib, weights_calib, 1 - alpha)\n",
    "        \n",
    "#         y_test_lb = model_quantile.predict(x_test, quantiles = alpha/2)\n",
    "#         y_test_ub = model_quantile.predict(x_test, quantiles = (1-alpha)/2)\n",
    "        \n",
    "        y_test_lb = qr_lb.predict(x_test)\n",
    "        y_test_ub = qr_ub.predict(x_test)\n",
    "        \n",
    "        width.append(2*quantile + y_test_ub - y_test_lb)\n",
    "        cov.append(y_test >= y_test_lb - quantile and y_test <= y_test_ub + quantile)\n",
    "\n",
    "    \n",
    "    return np.array(cov).mean(), np.array(width).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "533eaa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "def run_weighted_conformal_quantile(n):\n",
    "    cov, bw = weighted_conformal_quantile(Dtrain, Dshift, alpha)\n",
    "    return cov, bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0a494d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2032.105\n"
     ]
    }
   ],
   "source": [
    "cov, bw = run_weighted_conformal_quantile(1)\n",
    "print(cov)\n",
    "print(bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "82602aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 28)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dtrain.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
