{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66e3fe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from pygam import LinearGAM, s, f\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F # All functions that don't have any parameters\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "\n",
    "from scipy.stats import iqr\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as lines\n",
    "\n",
    "import warnings\n",
    "random_seed = 42\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"GPU is not available.\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e75fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generation(n, beta): \n",
    "    d = beta.shape[0]\n",
    "    X = np.random.uniform(-1, 1, (n, d))\n",
    "    variance_function = 1 + ((X @ beta) ** 2)\n",
    "    Y = X @ beta + (variance_function ** 0.5) * np.random.normal(n)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111da2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_generation_1dim(n): \n",
    "#     X = np.random.uniform(-1, 1, n)\n",
    "#     variance_function = 1 + np.abs(X ** 5)\n",
    "#     Y = 1 + X + (X ** 2) + (variance_function ** 0.5) * np.random.uniform(-1, 1, n)\n",
    "#     return X, Y\n",
    "\n",
    "def data_generation_1dim(n=2000):\n",
    "    \n",
    "    \"\"\"\n",
    "    i: random seed\n",
    "    n: number of samples\n",
    "    \"\"\"\n",
    "    \n",
    "    X = np.random.uniform(-1, 1, n).reshape(-1, 1)\n",
    "    Y = np.sqrt(1+25*np.power(X, 4))  * np.random.uniform(-1, 1, n).reshape(-1, 1)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e20838f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2500\n",
    "\n",
    "#gamma = np.array([-1, 0, 1, 1, 0])\n",
    "X, Y = data_generation_1dim(n)\n",
    "Data = np.hstack([X.reshape(-1, 1), Y.reshape(-1,1)])\n",
    "\n",
    "#beta = np.array([1, 0, -1, 2, 0])\n",
    "beta = 2\n",
    "\n",
    "def shifted_data_generation(Data, beta, prop = 0.75):\n",
    "    \n",
    "    # random split the data from source into two parts: training and testing data\n",
    "    np.random.shuffle(Data)\n",
    "    train_idx = int(Data.shape[0] * prop)\n",
    "    Dtrain, Dtest = np.split(Data, [train_idx])\n",
    "    \n",
    "    # creat the shift data set\n",
    "    probabilities = np.exp(Dtest[:,1:] * beta) # Calculate probabilities proportional to w(x) = exp(x^T * beta)\n",
    "    probabilities /= np.sum(probabilities) # Normalize probabilities\n",
    "    probabilities = probabilities.reshape(-1,)\n",
    "    \n",
    "    sample_indices = np.random.choice(len(Dtest), size=len(Dtest), replace=True, p=probabilities)\n",
    "    Dshift = Dtest[sample_indices]\n",
    "    \n",
    "    return Dtrain, Dtest, Dshift\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f9163ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN1(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NN1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 10)\n",
    "        self.fc2 = nn.Linear(10, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# 2-layer NN\n",
    "class NN2(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NN2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x \n",
    "    \n",
    "    \n",
    "    \n",
    "class QuantileLoss(nn.Module):\n",
    "    def __init__(self, quantile):\n",
    "        super().__init__()\n",
    "        self.quantile = quantile\n",
    "\n",
    "    def forward(self, preds, target):\n",
    "        assert not target.requires_grad\n",
    "        assert preds.size(0) == target.size(0)\n",
    "        residuals = target - preds\n",
    "        return torch.max((self.quantile - 1) * residuals, self.quantile * residuals).mean()\n",
    "        \n",
    "        \n",
    "def est_quantile(est_type,quantile,X_pre,Y_pre,X_opt,X_adj,X_t):\n",
    "    \"\"\"\n",
    "        est_type: \n",
    "        \"NN1\": 1-layer NN;              \"NN2\": 2-layer NN; \n",
    "        \"qrf\": quantile regression forest;    \"gb\": gradient boosting\n",
    "        \n",
    "        quantile: the quantile we are estimating\n",
    "        (X_pre,Y_pre): training data\n",
    "        X_opt,X_adj,X_t: data used to predict\n",
    "        output: quantile estimator Q and the prediction Q(X) \n",
    "    \"\"\"\n",
    "    \n",
    "    if est_type == \"NN1\":\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "        \n",
    "        # Convert numpy arrays to torch tensors\n",
    "        X = torch.from_numpy(X_pre).float().to(device)\n",
    "        Y = torch.from_numpy(Y_pre).float().to(device)\n",
    "        input_size = X.shape[1]\n",
    "        \n",
    "        model = NN1(input_size=input_size, output_size=1).to(device)\n",
    "        learning_rate = 0.001\n",
    "        \n",
    "        # Set loss function and optimizer\n",
    "        criterion = QuantileLoss(quantile).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(1000):\n",
    "            optimizer.zero_grad()   # zero the gradient buffers\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()    # Does the update\n",
    "            \n",
    "        # Predict\n",
    "        X_opt = torch.from_numpy(X_opt).float().to(device)\n",
    "        X_adj = torch.from_numpy(X_adj).float().to(device)\n",
    "        X_t = torch.from_numpy(X_t).float().to(device)\n",
    "\n",
    "        Q_opt = model(X_opt)\n",
    "        Q_opt = Q_opt.detach().cpu().numpy().reshape(-1,1)\n",
    "        Q_adj = model(X_adj)\n",
    "        Q_adj = Q_adj.detach().cpu().numpy().reshape(-1,1)\n",
    "        Q_t = model(X_t)\n",
    "        Q_t = Q_t.detach().cpu().numpy().reshape(-1,1)\n",
    "        return model, Q_opt, Q_adj, Q_t\n",
    "    \n",
    "    if est_type == \"NN2\":\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42) \n",
    "        \n",
    "        # Convert numpy arrays to torch tensors\n",
    "        X = torch.from_numpy(X_pre).float().to(device)\n",
    "        Y = torch.from_numpy(Y_pre).float().to(device)\n",
    "        input_size = X.shape[1]\n",
    "        \n",
    "        model = NN2(input_size=input_size, output_size=1).to(device)\n",
    "        learning_rate = 0.001\n",
    "        \n",
    "        # Set loss function and optimizer\n",
    "        criterion = QuantileLoss(quantile).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(1000):\n",
    "            optimizer.zero_grad()   # zero the gradient buffers\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()    # Does the update\n",
    "            \n",
    "        # Predict\n",
    "        X_opt = torch.from_numpy(X_opt).float().to(device)\n",
    "        X_adj = torch.from_numpy(X_adj).float().to(device)\n",
    "        X_t = torch.from_numpy(X_t).float().to(device)\n",
    "\n",
    "        Q_opt = model(X_opt)\n",
    "        Q_opt = Q_opt.detach().cpu().numpy().reshape(-1,1)\n",
    "        Q_adj = model(X_adj)\n",
    "        Q_adj = Q_adj.detach().cpu().numpy().reshape(-1,1)\n",
    "        Q_t = model(X_t)\n",
    "        Q_t = Q_t.detach().cpu().numpy().reshape(-1,1)\n",
    "        return model, Q_opt, Q_adj, Q_t\n",
    "    \n",
    "    if est_type == \"qrf\":\n",
    "        model = RandomForestQuantileRegressor(n_estimators = 500, random_state=random_seed)\n",
    "        model.fit(X_pre, Y_pre)\n",
    "        Q_opt = model.predict(X_opt,quantiles = [quantile]).reshape(-1,1)\n",
    "        Q_adj = model.predict(X_adj,quantiles = [quantile]).reshape(-1,1)\n",
    "        Q_t = model.predict(X_t,quantiles = [quantile]).reshape(-1,1)\n",
    "        return model, Q_opt, Q_adj, Q_t\n",
    "    \n",
    "    \n",
    "    if est_type == \"gb\":\n",
    "        model = GradientBoostingRegressor(n_estimators=300,random_state=random_seed,loss = \"quantile\", alpha = quantile)\n",
    "        model.fit(X_pre, Y_pre)\n",
    "        Q_opt = model.predict(X_opt).reshape(-1,1)\n",
    "        Q_adj = model.predict(X_adj).reshape(-1,1)\n",
    "        Q_t = model.predict(X_t).reshape(-1,1)\n",
    "        return model, Q_opt, Q_adj, Q_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d37a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtrain, Dtest, Dshift = shifted_data_generation(Data, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "445980a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_utopia(mc_iter):\n",
    "    Dtrain, Dtest, Dshift = shifted_data_generation(Data, beta)\n",
    "    \n",
    "    total_attributes = Dtrain.shape[1]\n",
    "    Y_train = Dtrain[:,0]\n",
    "    X_train = Dtrain[:,1:]\n",
    "\n",
    "    X_test = Dshift[:,1:]\n",
    "    Y_test = Dshift[:,0]\n",
    "\n",
    "\n",
    "    pre_idx = int(Dtrain.shape[0] * 0.5)\n",
    "    Dtrain_pre, Dtrain_opt = np.split(Dtrain, [pre_idx])\n",
    "    adj_idx = int(Dtrain_opt.shape[0] * 0.5)\n",
    "    Dtrain_opt, Dtrain_adj = np.split(Dtrain_opt, [adj_idx])\n",
    "\n",
    "    X_pre = Dtrain_pre[:,1:]\n",
    "    Y_pre = Dtrain_pre[:,0]\n",
    "\n",
    "    X_opt = Dtrain_opt[:,1:]\n",
    "    Y_opt = Dtrain_opt[:,0]\n",
    "\n",
    "    X_adj = Dtrain_adj[:,1:]\n",
    "    Y_adj = Dtrain_adj[:,0]\n",
    "\n",
    "    \n",
    "    mean_model = LinearRegression()\n",
    "    mean_model.fit(X_pre, Y_pre)\n",
    "    \n",
    "    ## change it to linear model \n",
    "\n",
    "    mean_est_pre = mean_model.predict(X_pre)\n",
    "    mean_est_opt = mean_model.predict(X_opt)\n",
    "    mean_est_adj = mean_model.predict(X_adj)\n",
    "\n",
    "    Y_centered_squared_pre = (Y_pre - mean_est_pre)**2 \n",
    "    Y_centered_squared_opt = (Y_opt - mean_est_opt)**2 \n",
    "    Y_centered_squared_adj = (Y_adj - mean_est_adj)**2 \n",
    "    \n",
    "    quantile = [0.85,0.95,0.9,0.9]\n",
    "\n",
    "\n",
    "    m1,Q1_opt,Q1_adj,Q1_t = est_quantile(\"NN1\",quantile[0],X_pre,Y_centered_squared_pre,X_opt,X_adj,X_test) # quantile def\n",
    "    m2,Q2_opt,Q2_adj,Q2_t = est_quantile(\"NN2\",quantile[1],X_pre,Y_centered_squared_pre,X_opt,X_adj,X_test)\n",
    "    m3,Q3_opt,Q3_adj,Q3_t = est_quantile(\"qrf\",quantile[2],X_pre,Y_centered_squared_pre,X_opt,X_adj,X_test)\n",
    "    m4,Q4_opt,Q4_adj,Q4_t = est_quantile(\"gb\",quantile[3],X_pre,Y_centered_squared_pre,X_opt,X_adj,X_test)\n",
    "    \n",
    "\n",
    "    ######## Variance estimator ###########\n",
    "    \n",
    "    rf_var_model = RandomForestRegressor(n_estimators = 1000, random_state = 42, max_depth = 20)\n",
    "    rf_var_model.fit(X_pre, Y_centered_squared_pre)\n",
    "    var_hat_pre = rf_var_model.predict(X_pre)\n",
    "    var_hat_adj = rf_var_model.predict(X_adj)\n",
    "    var_hat_opt = rf_var_model.predict(X_opt)\n",
    "    var_hat_test = rf_var_model.predict(X_test)\n",
    "    \n",
    "    E_opt = np.vstack((np.matrix.flatten(Q1_opt), np.matrix.flatten(Q2_opt), np.matrix.flatten(Q3_opt), np.matrix.flatten(Q4_opt), var_hat_opt))\n",
    "    E_adj = np.vstack((np.matrix.flatten(Q1_adj), np.matrix.flatten(Q2_adj), np.matrix.flatten(Q3_adj), np.matrix.flatten(Q4_adj), var_hat_adj))\n",
    "    E_test = np.vstack((np.matrix.flatten(Q1_t), np.matrix.flatten(Q2_t), np.matrix.flatten(Q3_t), np.matrix.flatten(Q4_t), var_hat_test))\n",
    "    \n",
    "    n_opt = X_opt.shape[0]\n",
    "    n_adj = X_adj.shape[0]\n",
    "    n_test = X_test.shape[0]\n",
    "\n",
    "    cons_opt = np.ones(n_opt).reshape(1,-1)\n",
    "    cons_adj = np.ones(n_adj).reshape(1,-1)\n",
    "    cons_test = np.ones(n_test).reshape(1,-1)\n",
    "\n",
    "    A_opt = np.vstack((E_opt,cons_opt))\n",
    "    A_adj = np.vstack((E_adj,cons_adj))\n",
    "    A_test = np.vstack((E_test,cons_test))\n",
    "\n",
    "    weight = cp.Variable(A_opt.shape[0])\n",
    "\n",
    "\n",
    "    constraints = [weight>=0]+[weight @ A_opt >= Y_centered_squared_opt]  ### Change the objective \n",
    "    prob = cp.Problem(cp.Minimize(cp.sum(weight @ A_test)), constraints)\n",
    "    prob.solve()\n",
    "    optimal_weight = weight.value\n",
    "\n",
    "    f_hat_init_opt = optimal_weight @  A_opt\n",
    "    f_hat_init_adj = optimal_weight @  A_adj\n",
    "    f_hat_init_test = optimal_weight @ A_test\n",
    "    \n",
    "    f_hat_init_adj[f_hat_init_adj < 0] = 0\n",
    "    f_hat_init_test[f_hat_init_test < 0] = 0\n",
    "    \n",
    "    domain_cov = np.vstack((X_pre, X_test))\n",
    "    domain_idx =  np.vstack((np.zeros(X_pre.shape[0]).reshape(-1,1), np.ones(X_test.shape[0]).reshape(-1,1)))\n",
    "    domain_idx = np.matrix.flatten(domain_idx)\n",
    "\n",
    "    logistic_model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000,random_state=0) \n",
    "    logistic_model.fit(domain_cov, domain_idx)\n",
    "\n",
    "    prob_hat = logistic_model.predict_proba(X_adj)[:,0]\n",
    "    prob_hat = np.clip(prob_hat, 0.01, 0.99)\n",
    "    est_ratio = prob_hat/(1-prob_hat) #### Multiplying by the constant or normalization #####\n",
    "    #est_ratio /= est_ratio.mean()\n",
    "    est_ratio = est_ratio * (X_pre.shape[0]/X_test.shape[0])\n",
    "    \n",
    "    \n",
    "    delta = 1.01\n",
    "    alpha = 0.05\n",
    "    stepsize = 0.005\n",
    "    prop_outside = np.mean(est_ratio * ((Y_centered_squared_adj/(f_hat_init_adj + 1e-10)) > delta))\n",
    "    while prop_outside <= alpha:\n",
    "        delta = delta - stepsize\n",
    "        prop_outside = np.mean(est_ratio * ((Y_centered_squared_adj/(f_hat_init_adj + 1e-10)) > delta))\n",
    "        \n",
    "\n",
    "    mean_est_test = mean_model.predict(X_test)\n",
    "    Y_centered_squared_test = (Y_test - mean_est_test)**2 \n",
    "    cov = np.mean(Y_centered_squared_test < (delta * f_hat_init_test))\n",
    "    bw = 2 * np.mean((delta * f_hat_init_test) ** 0.5)\n",
    "    return cov, bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a6276220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "results_utopia = Parallel(n_jobs=10)(delayed(run_utopia)(mc_iter) for mc_iter in range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "abf6dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_utopia = np.array(results_utopia)[:,0]\n",
    "bandwidth_utopia = np.array(results_utopia)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "113975b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8855999999999999\n",
      "2.0908599852972594\n",
      "2.0908599852972594\n",
      "0.2023999999999999\n",
      "0.11267225571728678\n"
     ]
    }
   ],
   "source": [
    "print(np.median(coverage_utopia))\n",
    "print(np.median(bandwidth_utopia))\n",
    "print(np.median(bandwidth_utopia))\n",
    "print(iqr(coverage_utopia))\n",
    "print(iqr(bandwidth_utopia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e1d5fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9344, 0.8176, 0.6736, 0.816 , 0.7264, 0.6592, 0.7648, 0.7488,\n",
       "       0.4128, 0.7616, 0.2976, 0.7136, 0.808 , 0.72  , 0.7312, 0.5312,\n",
       "       0.4   , 0.8512, 0.6736, 0.6272, 0.8304, 0.7536, 0.7424, 0.6416,\n",
       "       0.6656, 0.6544, 0.8608, 0.3824, 0.7888, 0.816 , 0.7824, 0.2528,\n",
       "       0.632 , 0.5504, 0.7008, 0.6848, 0.7232, 0.8176, 0.2272, 0.9952,\n",
       "       0.56  , 0.4   , 0.3232, 0.6208, 0.4016, 0.8976, 0.7664, 0.4464,\n",
       "       0.7456, 0.8448, 0.6672, 0.8656, 0.8112, 0.608 , 0.6496, 0.6896,\n",
       "       0.5504, 0.4656, 0.64  , 0.9216, 0.4032, 0.8304, 0.7776, 0.4048,\n",
       "       0.7072, 0.8928, 0.8224, 0.9264, 0.8768, 0.3872, 0.44  , 0.8016,\n",
       "       0.8128, 0.4688, 0.5216, 0.8384, 0.8976, 0.5408, 0.7712, 0.7664,\n",
       "       0.7792, 0.7824, 0.7568, 0.5424, 0.7824, 0.6656, 0.8544, 0.7024,\n",
       "       0.7872, 0.6736, 0.608 , 0.5696, 0.6368, 0.816 , 0.4208, 0.9664,\n",
       "       0.4944, 0.7936, 0.768 , 0.752 ])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverage_utopia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7716b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Weighted Conformal Prediction ##########################\n",
    "\n",
    "def prob_est(Dtrain, Dshift, est_type = 'lr'):\n",
    "    X_S = Dtrain[:,1:]\n",
    "    X_T = Dshift[:,1:]\n",
    "    n_S = X_S.shape[0]\n",
    "    n_T = X_T.shape[0]\n",
    "    Y_S = np.zeros(n_S).reshape(-1,1)\n",
    "    Y_T = np.ones(n_T).reshape(-1,1)\n",
    "    X_train = np.vstack((X_S, X_T))\n",
    "    Y_train = np.vstack((Y_S, Y_T))\n",
    "  \n",
    "    if est_type == 'lr':\n",
    "        # fit logistic regression\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            logistic_model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000,random_state=0) \n",
    "            logistic_model.fit(X_train, Y_train[:,0])\n",
    "\n",
    "        return logistic_model\n",
    "    \n",
    "    if est_type == 'rf':\n",
    "        # fit random forest\n",
    "        rf_model = RandomForestClassifier(n_estimators = 100, random_state=random_seed)\n",
    "        rf_model.fit(X_train, Y_train[:,0])\n",
    "\n",
    "        return rf_model\n",
    "\n",
    "    \n",
    "def ratio_est(model, X):\n",
    "    \n",
    "        prob_1 = model.predict_proba(X)[:,1]\n",
    "        prob_1 = np.clip(prob_1, 0.01, 0.99)\n",
    "        est_ratio = prob_1/(1-prob_1)\n",
    "        \n",
    "        return est_ratio\n",
    "    \n",
    "def weighting_function(model, X_calib, x_test, x):\n",
    "    # Define weighting function here\n",
    "    # all the data should be array\n",
    "    ratio_calib = ratio_est(model, X_calib).sum()\n",
    "    ratio_test = ratio_est(model, x_test)\n",
    "    ratio_sum = ratio_calib + ratio_test\n",
    "    ratio_x = ratio_est(model, x)\n",
    "    return ratio_x/ratio_sum\n",
    "\n",
    "\n",
    "def weighted_quantile(values, weights, quantile):\n",
    "    \n",
    "    \"\"\" Compute the weighted quantile of a 1D numpy array.\n",
    "    \"\"\"\n",
    "    values = np.array(values)\n",
    "    weights = np.array(weights)\n",
    "    \n",
    "    sorter = np.argsort(values)\n",
    "    values = values[sorter]\n",
    "    weights = weights[sorter]\n",
    "    cumulative_weights = np.cumsum(weights)\n",
    "    if cumulative_weights[-1]>= quantile:\n",
    "        idx = np.argmax(cumulative_weights >= quantile)\n",
    "        return values[idx]\n",
    "    else:\n",
    "        return np.max(values)\n",
    "\n",
    "    \n",
    "    \n",
    "def weighted_conformal_cov_width(Dtrain, Dshift, alpha=0.05):\n",
    "    \n",
    "    n = Dshift.shape[0]\n",
    "    cov = []\n",
    "    width = []\n",
    "    \n",
    "    \n",
    "    prob_est_model = prob_est(Dtrain, Dshift, est_type = 'lr')\n",
    "    \n",
    "    # split weighted conformal\n",
    "    D = Dtrain\n",
    "    np.random.shuffle(D)\n",
    "    split_idx = int(D.shape[0] * 0.5)\n",
    "    D1, D2 = np.split(D, [split_idx])\n",
    "    X1 = D1[:,1:]\n",
    "    y1 = D1[:,0]\n",
    "    X2 = D2[:,1:]\n",
    "    y2 = D2[:,0]\n",
    "    \n",
    "    # Train the model on the first part of the data\n",
    "#     model = LinearRegression()\n",
    "#     model.fit(X1, y1)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X1, y1)\n",
    "    \n",
    "    # Compute the model's predictions for the calibration set\n",
    "    y1_hat = model.predict(X1)\n",
    "    y_calib_pred = model.predict(X2)\n",
    "            \n",
    "    # Calculate the absolute errors on the calibration set\n",
    "    errors_calib = np.abs(y_calib_pred - y2)\n",
    "    \n",
    "    est_error_1 = (y1_hat - y1) ** 2\n",
    "    var_model = RandomForestRegressor(n_estimators = 1000, random_state = 42, max_depth = 20)\n",
    "    var_model.fit(X1, est_error_1)\n",
    "    var_hat = var_model.predict(X2)\n",
    "    sd_calib_pred = var_hat ** 0.5\n",
    "    errors_calib = np.abs(y_calib_pred - y2)/np.clip(sd_calib_pred, 0.001, 1e10)\n",
    "    \n",
    "    for i in range(n):\n",
    "        x_test = Dshift[i,1:]\n",
    "        x_test = np.array(x_test).reshape(1,-1)\n",
    "        y_test = Dshift[i,0]\n",
    "    \n",
    "    \n",
    "        # Calculate the weights for the calibration set\n",
    "        weights_calib = weighting_function(prob_est_model, X2, x_test, X2)\n",
    "\n",
    "\n",
    "        # Calculate the weighted quantile of the errors\n",
    "        quantile = weighted_quantile(errors_calib, weights_calib, 1 - alpha)\n",
    "\n",
    "        # Now for the test\n",
    "        y_test_pred = model.predict(x_test)\n",
    "        sd_test_pred = var_model.predict(x_test) ** 0.5\n",
    "        \n",
    "        width.append(2*quantile*sd_test_pred)\n",
    "        cov.append((np.abs(y_test - y_test_pred)/sd_test_pred) <= quantile)\n",
    "\n",
    "    \n",
    "    return np.array(cov).mean(), np.array(width).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f51538ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "def run_weighted_conformal(n):\n",
    "    Dtrain, Dtest, Dshift = shifted_data_generation(Data,beta)\n",
    "    cov, bw = weighted_conformal_cov_width(Dtrain, Dshift, alpha)\n",
    "    return cov, bw\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "results_wc_heterogeneous = Parallel(n_jobs=10)(delayed(run_weighted_conformal)(i) for i in range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "53936309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736\n",
      "5.000832686224179\n",
      "5.161716797214706\n",
      "0.025999999999999912\n",
      "2.273149799511586\n"
     ]
    }
   ],
   "source": [
    "coverage_wc = np.array(results_wc_heterogeneous)[:,0]\n",
    "bandwidth_wc = np.array(results_wc_heterogeneous)[:,1]\n",
    "\n",
    "print(np.median(coverage_wc))\n",
    "print(np.median(bandwidth_wc))\n",
    "print(np.median(bandwidth_wc[coverage_wc > 0.95]))\n",
    "print(iqr(coverage_wc))\n",
    "print(iqr(bandwidth_wc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
