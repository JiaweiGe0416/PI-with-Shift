{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "923be13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F # All functions that don't have any parameters\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "from scipy.stats import iqr\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "\n",
    "\n",
    "\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as lines\n",
    "\n",
    "import ot\n",
    "\n",
    "import warnings\n",
    "random_seed = 42\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"GPU is not available.\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a3e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sdata = pd.read_csv('ETTh1.csv', header = None)\n",
    "SData = Sdata.to_numpy()\n",
    "SData = SData[1:,1:]\n",
    "SData = np.array(SData, dtype=float)\n",
    "SData = SData[0:3000,:]\n",
    "\n",
    "Tdata = pd.read_csv('ETTh2.csv', header = None)\n",
    "TData = Tdata.to_numpy()\n",
    "TData = TData[1:,1:]\n",
    "TData = np.array(TData, dtype=float)\n",
    "TData = TData[0:3000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "758971d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN1(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NN1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 10)\n",
    "        self.fc2 = nn.Linear(10, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# 2-layer NN\n",
    "class NN2(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NN2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x \n",
    "    \n",
    "    \n",
    "    \n",
    "class QuantileLoss(nn.Module):\n",
    "    def __init__(self, quantile):\n",
    "        super().__init__()\n",
    "        self.quantile = quantile\n",
    "\n",
    "    def forward(self, preds, target):\n",
    "        assert not target.requires_grad\n",
    "        assert preds.size(0) == target.size(0)\n",
    "        residuals = target - preds\n",
    "        return torch.max((self.quantile - 1) * residuals, self.quantile * residuals).mean()\n",
    "        \n",
    "        \n",
    "def est_quantile(est_type,quantile,X_pre,Y_pre,X_opt,X_adj,X_t):\n",
    "    \"\"\"\n",
    "        est_type: \n",
    "        \"NN1\": 1-layer NN;              \"NN2\": 2-layer NN; \n",
    "        \"qrf\": quantile regression forest;    \"gb\": gradient boosting\n",
    "        \n",
    "        quantile: the quantile we are estimating\n",
    "        (X_pre,Y_pre): training data\n",
    "        X_opt,X_adj,X_t: data used to predict\n",
    "        output: quantile estimator Q and the prediction Q(X) \n",
    "    \"\"\"\n",
    "    \n",
    "    num_var = X_pre.shape[1]\n",
    "    \n",
    "    if est_type == \"NN1\":\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "        \n",
    "        # Convert numpy arrays to torch tensors\n",
    "        X = torch.from_numpy(X_pre).float().to(device)\n",
    "        Y = torch.from_numpy(Y_pre).float().to(device)\n",
    "        \n",
    "        model = NN1(input_size=num_var, output_size=1).to(device)\n",
    "        learning_rate = 0.001\n",
    "        \n",
    "        # Set loss function and optimizer\n",
    "        criterion = QuantileLoss(quantile).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(1000):\n",
    "            optimizer.zero_grad()   # zero the gradient buffers\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()    # Does the update\n",
    "            \n",
    "        # Predict\n",
    "        X_opt = torch.from_numpy(X_opt).float().to(device)\n",
    "        X_adj = torch.from_numpy(X_adj).float().to(device)\n",
    "        X_t = torch.from_numpy(X_t).float().to(device)\n",
    "\n",
    "        Q_opt = model(X_opt)\n",
    "        Q_opt = Q_opt.detach().cpu().numpy().reshape(-1,1)\n",
    "        Q_adj = model(X_adj)\n",
    "        Q_adj = Q_adj.detach().cpu().numpy().reshape(-1,1)\n",
    "        Q_t = model(X_t)\n",
    "        Q_t = Q_t.detach().cpu().numpy().reshape(-1,1)\n",
    "        return model, Q_opt, Q_adj, Q_t\n",
    "    \n",
    "    if est_type == \"NN2\":\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42) \n",
    "        \n",
    "        # Convert numpy arrays to torch tensors\n",
    "        X = torch.from_numpy(X_pre).float().to(device)\n",
    "        Y = torch.from_numpy(Y_pre).float().to(device)\n",
    "        \n",
    "        model = NN2(input_size=num_var, output_size=1).to(device)\n",
    "        learning_rate = 0.001\n",
    "        \n",
    "        # Set loss function and optimizer\n",
    "        criterion = QuantileLoss(quantile).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(1000):\n",
    "            optimizer.zero_grad()   # zero the gradient buffers\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()    # Does the update\n",
    "            \n",
    "        # Predict\n",
    "        X_opt = torch.from_numpy(X_opt).float().to(device)\n",
    "        X_adj = torch.from_numpy(X_adj).float().to(device)\n",
    "        X_t = torch.from_numpy(X_t).float().to(device)\n",
    "\n",
    "        Q_opt = model(X_opt)\n",
    "        Q_opt = Q_opt.detach().cpu().numpy().reshape(-1,1)\n",
    "        Q_adj = model(X_adj)\n",
    "        Q_adj = Q_adj.detach().cpu().numpy().reshape(-1,1)\n",
    "        Q_t = model(X_t)\n",
    "        Q_t = Q_t.detach().cpu().numpy().reshape(-1,1)\n",
    "        return model, Q_opt, Q_adj, Q_t\n",
    "    \n",
    "    if est_type == \"qrf\":\n",
    "        model = RandomForestQuantileRegressor(n_estimators = 500, random_state=random_seed)\n",
    "        model.fit(X_pre, Y_pre)\n",
    "        Q_opt = model.predict(X_opt,quantiles = [quantile]).reshape(-1,1)\n",
    "        Q_adj = model.predict(X_adj,quantiles = [quantile]).reshape(-1,1)\n",
    "        Q_t = model.predict(X_t,quantiles = [quantile]).reshape(-1,1)\n",
    "        return model, Q_opt, Q_adj, Q_t\n",
    "    \n",
    "    \n",
    "    if est_type == \"gb\":\n",
    "        model = GradientBoostingRegressor(n_estimators=300,random_state=random_seed,loss = \"quantile\", alpha = quantile)\n",
    "        model.fit(X_pre, Y_pre)\n",
    "        Q_opt = model.predict(X_opt).reshape(-1,1)\n",
    "        Q_adj = model.predict(X_adj).reshape(-1,1)\n",
    "        Q_t = model.predict(X_t).reshape(-1,1)\n",
    "        return model, Q_opt, Q_adj, Q_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0072ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_utopia(mc_iter):\n",
    "    Dtrain = SData\n",
    "    Dshift = TData\n",
    "    Y_train = Dtrain[:,-1]\n",
    "    X_train = Dtrain[:,:-1]\n",
    "\n",
    "    X_test = Dshift[:,:-1]\n",
    "    Y_test = Dshift[:,-1]\n",
    "\n",
    "\n",
    "    pre_idx = int(Dtrain.shape[0] * 0.5)\n",
    "    Dtrain_pre, Dtrain_opt = np.split(Dtrain, [pre_idx])\n",
    "    adj_idx = int(Dtrain_opt.shape[0] * 0.5)\n",
    "    Dtrain_opt, Dtrain_adj = np.split(Dtrain_opt, [adj_idx])\n",
    "\n",
    "    X_pre = Dtrain_pre[:,:-1]\n",
    "    Y_pre = Dtrain_pre[:,-1]\n",
    "\n",
    "    X_opt = Dtrain_opt[:,:-1]\n",
    "    Y_opt = Dtrain_opt[:,-1]\n",
    "\n",
    "    X_adj = Dtrain_adj[:,:-1]\n",
    "    Y_adj = Dtrain_adj[:,-1]\n",
    "    \n",
    "    #mean_model = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "    mean_model = LinearRegression()\n",
    "    mean_model.fit(X_pre, Y_pre)\n",
    "\n",
    "    mean_est_pre = mean_model.predict(X_pre)\n",
    "    mean_est_opt = mean_model.predict(X_opt)\n",
    "    mean_est_adj = mean_model.predict(X_adj)\n",
    "\n",
    "    Y_centered_squared_pre = (Y_pre - mean_est_pre)**2 \n",
    "    Y_centered_squared_opt = (Y_opt - mean_est_opt)**2 \n",
    "    Y_centered_squared_adj = (Y_adj - mean_est_adj)**2 \n",
    "    \n",
    "    \n",
    "    ######## Estimation of the shift ###########\n",
    "#     ot_sinkhorn = ot.da.SinkhornTransport(reg_e=1e-1, max_iter=1000)\n",
    "#     ot_sinkhorn.fit(Xs=X_test, Xt=X_pre)\n",
    "#     X_test = ot_sinkhorn.transform(Xs=X_test)\n",
    "    \n",
    "    ot_emd = ot.da.EMDTransport()\n",
    "    ot_emd.fit(Xs=X_test, Xt=X_pre)\n",
    "    X_test = ot_emd.transform(Xs=X_test)\n",
    "    \n",
    "#     ot_emd = ot.da.LinearTransport()\n",
    "#     ot_emd.fit(Xs=X_test, Xt=X_pre)\n",
    "#     X_test = ot_emd.transform(Xs=X_test)\n",
    "    \n",
    "    \n",
    "    quantile = [0.85,0.95,0.9,0.9]\n",
    "\n",
    "\n",
    "    m1,Q1_opt,Q1_adj,Q1_t = est_quantile(\"NN1\",quantile[0],X_pre,Y_centered_squared_pre,X_opt,X_adj,X_test) # quantile def\n",
    "    m2,Q2_opt,Q2_adj,Q2_t = est_quantile(\"NN2\",quantile[1],X_pre,Y_centered_squared_pre,X_opt,X_adj,X_test)\n",
    "    m3,Q3_opt,Q3_adj,Q3_t = est_quantile(\"qrf\",quantile[2],X_pre,Y_centered_squared_pre,X_opt,X_adj,X_test)\n",
    "    m4,Q4_opt,Q4_adj,Q4_t = est_quantile(\"gb\",quantile[3],X_pre,Y_centered_squared_pre,X_opt,X_adj,X_test)\n",
    "    \n",
    "\n",
    "    ######## Variance estimator ###########\n",
    "    \n",
    "    rf_var_model = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "    rf_var_model.fit(X_pre, Y_centered_squared_pre)\n",
    "    var_hat_pre = rf_var_model.predict(X_pre)\n",
    "    var_hat_adj = rf_var_model.predict(X_adj)\n",
    "    var_hat_opt = rf_var_model.predict(X_opt)\n",
    "    var_hat_test = rf_var_model.predict(X_test)\n",
    "    \n",
    "    E_opt = np.vstack((np.matrix.flatten(Q1_opt), np.matrix.flatten(Q2_opt), np.matrix.flatten(Q3_opt), np.matrix.flatten(Q4_opt), var_hat_opt))\n",
    "    E_adj = np.vstack((np.matrix.flatten(Q1_adj), np.matrix.flatten(Q2_adj), np.matrix.flatten(Q3_adj), np.matrix.flatten(Q4_adj), var_hat_adj))\n",
    "    E_test = np.vstack((np.matrix.flatten(Q1_t), np.matrix.flatten(Q2_t), np.matrix.flatten(Q3_t), np.matrix.flatten(Q4_t), var_hat_test))\n",
    "    \n",
    "    n_opt = X_opt.shape[0]\n",
    "    n_adj = X_adj.shape[0]\n",
    "    n_test = X_test.shape[0]\n",
    "\n",
    "    cons_opt = np.ones(n_opt).reshape(1,-1)\n",
    "    cons_adj = np.ones(n_adj).reshape(1,-1)\n",
    "    cons_test = np.ones(n_test).reshape(1,-1)\n",
    "\n",
    "    A_opt = np.vstack((E_opt,cons_opt))\n",
    "    A_adj = np.vstack((E_adj,cons_adj))\n",
    "    A_test = np.vstack((E_test,cons_test))\n",
    "\n",
    "    weight = cp.Variable(A_opt.shape[0])\n",
    "\n",
    "\n",
    "    constraints = [weight>=0]+[weight @ A_opt >= Y_centered_squared_opt]  ### Change the objective \n",
    "    prob = cp.Problem(cp.Minimize(cp.sum(weight @ A_opt)), constraints)\n",
    "    prob.solve()\n",
    "    optimal_weight = weight.value\n",
    "\n",
    "    f_hat_init_opt = optimal_weight @  A_opt\n",
    "    f_hat_init_adj = optimal_weight @  A_adj\n",
    "    f_hat_init_test = optimal_weight @ A_test\n",
    "\n",
    "        \n",
    "    \n",
    "    alpha = 0.05\n",
    "    scores_temp = Y_centered_squared_adj/f_hat_init_adj\n",
    "    level = (1 - alpha) * 100\n",
    "    level = int(level)\n",
    "    delta = np.percentile(scores_temp, level)\n",
    "    \n",
    "    \n",
    "    mean_est_test = mean_model.predict(X_test)\n",
    "    Y_centered_squared_test = (Y_test - mean_est_test)**2 \n",
    "    cov = np.mean(Y_centered_squared_test < (delta * f_hat_init_test))\n",
    "    bw = 2 * np.mean((delta * f_hat_init_test) ** 0.5)\n",
    "    return cov, bw, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e6fd530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdeb/anaconda3/lib/python3.10/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/Users/mdeb/anaconda3/lib/python3.10/site-packages/ot/da.py:594: RuntimeWarning: invalid value encountered in divide\n",
      "  transp = self.coupling_ / nx.sum(self.coupling_, axis=1)[:, None]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976\n",
      "41.524866061296855\n"
     ]
    }
   ],
   "source": [
    "cov,bw,delta = run_utopia(1)\n",
    "print(cov)\n",
    "print(bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "929671f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Weighted variance adjusted split conformal #######################################\n",
    "\n",
    "def prob_est(Dtrain, Dshift, est_type = 'lr'):\n",
    "    X_S = Dtrain[:,:-1]\n",
    "    X_T = Dshift[:,:-1]\n",
    "    n_S = X_S.shape[0]\n",
    "    n_T = X_T.shape[0]\n",
    "    Y_S = np.zeros(n_S).reshape(-1,1)\n",
    "    Y_T = np.ones(n_T).reshape(-1,1)\n",
    "    X_train = np.vstack((X_S, X_T))\n",
    "    Y_train = np.vstack((Y_S, Y_T))\n",
    "  \n",
    "    if est_type == 'lr':\n",
    "        # fit logistic regression\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            logistic_model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000,random_state=0) \n",
    "            logistic_model.fit(X_train, Y_train[:,0])\n",
    "\n",
    "        return logistic_model\n",
    "    \n",
    "    if est_type == 'rf':\n",
    "        # fit random forest\n",
    "        rf_model = RandomForestClassifier(n_estimators = 100, random_state=random_seed)\n",
    "        rf_model.fit(X_train, Y_train[:,0])\n",
    "\n",
    "        return rf_model\n",
    "\n",
    "    \n",
    "def ratio_est(model, X):\n",
    "    \n",
    "        prob_1 = model.predict_proba(X)[:,1]\n",
    "        prob_1 = np.clip(prob_1, 0.01, 0.99)\n",
    "        est_ratio = prob_1/(1-prob_1)\n",
    "        \n",
    "        return est_ratio\n",
    "    \n",
    "def weighting_function(model, X_calib, x_test, x):\n",
    "    # Define weighting function here\n",
    "    # all the data should be array\n",
    "    ratio_calib = ratio_est(model, X_calib).sum()\n",
    "    ratio_test = ratio_est(model, x_test)\n",
    "    ratio_sum = ratio_calib + ratio_test\n",
    "    ratio_x = ratio_est(model, x)\n",
    "    return ratio_x/ratio_sum\n",
    "\n",
    "\n",
    "def weighted_quantile(values, weights, quantile):\n",
    "    \n",
    "    \"\"\" Compute the weighted quantile of a 1D numpy array.\n",
    "    \"\"\"\n",
    "    values = np.array(values)\n",
    "    weights = np.array(weights)\n",
    "    \n",
    "    sorter = np.argsort(values)\n",
    "    values = values[sorter]\n",
    "    weights = weights[sorter]\n",
    "    cumulative_weights = np.cumsum(weights)\n",
    "    if cumulative_weights[-1]>= quantile:\n",
    "        idx = np.argmax(cumulative_weights >= quantile)\n",
    "        return values[idx]\n",
    "    else:\n",
    "        return np.max(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a767aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_conformal_cov_width(Dtrain, Dshift, alpha=0.05):\n",
    "    \n",
    "    n = Dshift.shape[0]\n",
    "    cov = []\n",
    "    width = []\n",
    "    \n",
    "    \n",
    "    prob_est_model = prob_est(Dtrain, Dshift, est_type = 'lr')\n",
    "    \n",
    "    # split weighted conformal\n",
    "    D = Dtrain\n",
    "    np.random.shuffle(D)\n",
    "    split_idx = int(D.shape[0] * 0.5)\n",
    "    D1, D2 = np.split(D, [split_idx])\n",
    "    X1 = D1[:,:-1]\n",
    "    y1 = D1[:,-1]\n",
    "    X2 = D2[:,:-1]\n",
    "    y2 = D2[:,-1]\n",
    "    \n",
    "    # Train the model on the first part of the data\n",
    "    model = LinearRegression()\n",
    "    model.fit(X1, y1)\n",
    "    \n",
    "    # Compute the model's predictions for the calibration set\n",
    "    y1_hat = model.predict(X1)\n",
    "    y_calib_pred = model.predict(X2)\n",
    "            \n",
    "    # Calculate the absolute errors on the calibration set\n",
    "    errors_calib = np.abs(y_calib_pred - y2)\n",
    "    \n",
    "    est_error_1 = (y1_hat - y1) ** 2\n",
    "    var_model = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "    var_model.fit(X1, est_error_1)\n",
    "    var_hat = var_model.predict(X2)\n",
    "    sd_calib_pred = var_hat ** 0.5\n",
    "    errors_calib = np.abs(y_calib_pred - y2)/np.clip(sd_calib_pred, 0.001, 1e10)\n",
    "    \n",
    "    for i in range(n):\n",
    "        x_test = Dshift[i,:-1]\n",
    "        x_test = np.array(x_test).reshape(1,-1)\n",
    "        y_test = Dshift[i,-1]\n",
    "    \n",
    "    \n",
    "        # Calculate the weights for the calibration set\n",
    "        weights_calib = weighting_function(prob_est_model, X2, x_test, X2)\n",
    "\n",
    "\n",
    "        # Calculate the weighted quantile of the errors\n",
    "        quantile = weighted_quantile(errors_calib, weights_calib, 1 - alpha)\n",
    "\n",
    "        # Now for the test\n",
    "        y_test_pred = model.predict(x_test)\n",
    "        sd_test_pred = var_model.predict(x_test) ** 0.5\n",
    "        \n",
    "        width.append(2*quantile*sd_test_pred)\n",
    "        cov.append((np.abs(y_test - y_test_pred)/sd_test_pred) <= quantile)\n",
    "\n",
    "    \n",
    "    return np.array(cov).mean(), np.array(width).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "353dc47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "def run_weighted_conformal(n):\n",
    "    Dtrain = SData\n",
    "    Dshift = TData\n",
    "    cov, bw = weighted_conformal_cov_width(Dtrain, Dshift, alpha)\n",
    "    return cov, bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48165971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9816666666666667\n",
      "57.90698316455262\n"
     ]
    }
   ],
   "source": [
    "cov, bw = run_weighted_conformal(1)\n",
    "print(cov)\n",
    "print(bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ed0dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_conformal_quantile(Dtrain, Dshift, alpha=0.05):\n",
    "    \n",
    "    n = Dshift.shape[0]\n",
    "    cov = []\n",
    "    width = []\n",
    "    \n",
    "    \n",
    "    prob_est_model = prob_est(Dtrain, Dshift, est_type = 'lr')\n",
    "    \n",
    "    # split weighted conformal\n",
    "    D = Dtrain\n",
    "    np.random.shuffle(D)\n",
    "    split_idx = int(D.shape[0] * 0.5)\n",
    "    D1, D2 = np.split(D, [split_idx])\n",
    "    X1 = D1[:,:-1]\n",
    "    y1 = D1[:,-1]\n",
    "    X2 = D2[:,:-1]\n",
    "    y2 = D2[:,-1]\n",
    "    \n",
    "    # Train the model on the first part of the data\n",
    "#     model_quantile = RandomForestQuantileRegressor(n_estimators = 500, random_state=random_seed)\n",
    "#     model_quantile.fit(X1, y1)\n",
    "    \n",
    "    qr_lb = QuantileRegressor(quantile=alpha/2, solver = 'highs')\n",
    "    qr_ub = QuantileRegressor(quantile = (1-alpha)/2, solver = 'highs')\n",
    "    \n",
    "    qr_lb.fit(X1, y1)\n",
    "    qr_ub.fit(X1, y1)\n",
    "    \n",
    "#     y2_lb_hat = model_quantile.predict(X2, quantiles = alpha/2)\n",
    "#     y2_ub_hat = model_quantile.predict(X2, quantiles = (1-alpha)/2)\n",
    "    \n",
    "    y2_lb_hat = qr_lb.predict(X2)\n",
    "    y2_ub_hat = qr_ub.predict(X2)\n",
    "        \n",
    "#     model_ub = QuantileRegressor(quantile = (1-alpha)/2, alpha = 1, fit_intercept=True, solver='highs', solver_options=None)\n",
    "#     model_ub.fit(X1, y1)\n",
    "    \n",
    "#     model_lb = QuantileRegressor(quantile = alpha/2, alpha = 1, fit_intercept=True, solver='highs', solver_options=None)\n",
    "#     model_lb.fit(X1, y1)\n",
    "    \n",
    "#     y2_lb_hat = model_lb.predict(X2)\n",
    "#     y2_ub_hat = model_ub.predict(X2)\n",
    "    \n",
    "    errors_calib = np.maximum(y2_lb_hat - y2, y2 - y2_ub_hat)\n",
    "    \n",
    "    for i in range(n):\n",
    "        x_test = Dshift[i,:-1]\n",
    "        x_test = np.array(x_test).reshape(1,-1)\n",
    "        y_test = Dshift[i,-1]\n",
    "    \n",
    "    \n",
    "        # Calculate the weights for the calibration set\n",
    "        weights_calib = weighting_function(prob_est_model, X2, x_test, X2)\n",
    "\n",
    "\n",
    "        # Calculate the weighted quantile of the errors\n",
    "        quantile = weighted_quantile(errors_calib, weights_calib, 1 - alpha)\n",
    "        \n",
    "#         y_test_lb = model_quantile.predict(x_test, quantiles = alpha/2)\n",
    "#         y_test_ub = model_quantile.predict(x_test, quantiles = (1-alpha)/2)\n",
    "        \n",
    "        y_test_lb = qr_lb.predict(x_test)\n",
    "        y_test_ub = qr_ub.predict(x_test)\n",
    "        \n",
    "        width.append(2*quantile + y_test_ub - y_test_lb)\n",
    "        cov.append(y_test >= y_test_lb - quantile and y_test <= y_test_ub + quantile)\n",
    "\n",
    "    \n",
    "    return np.array(cov).mean(), np.array(width).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19085e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "def run_weighted_conformal_quantile(n):\n",
    "    Dtrain = SData\n",
    "    Dshift = TData\n",
    "    cov, bw = weighted_conformal_quantile(Dtrain, Dshift, alpha)\n",
    "    return cov, bw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ee45253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8416666666666667\n",
      "54.98122101338704\n"
     ]
    }
   ],
   "source": [
    "cov, bw = run_weighted_conformal_quantile(1)\n",
    "print(cov)\n",
    "print(bw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
